{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.merge import add\n",
    "\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "## Encoder\n",
    "The role of this model is to create feature vector from raw images.\n",
    "\n",
    "Borrow from https://github.com/uzh-rpg/rpg_public_dronet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet8(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet8, self).__init__(name='')\n",
    "        \n",
    "        self.conv2d = Conv2D(32, (5, 5), strides=[2,2], padding='same')\n",
    "        self.maxpool = MaxPooling2D(pool_size=(3, 3), strides=[2,2])\n",
    "\n",
    "        # First residual block\n",
    "        self.r1_bn_1 = keras.layers.normalization.BatchNormalization()\n",
    "        self.r1_act_1 = Activation('relu')\n",
    "        self.r1_conv2d_1 = Conv2D(32, (3, 3), strides=[2,2], padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "        self.r1_bn_2 = keras.layers.normalization.BatchNormalization()\n",
    "        self.r1_act_2 = Activation('relu')\n",
    "        self.r1_conv2d_2 = Conv2D(32, (3, 3), padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "        self.r1_conv2d_3 = Conv2D(32, (1, 1), strides=[2,2], padding='same')\n",
    "        #x3 = add([x1, x2])\n",
    "\n",
    "        # Second residual block\n",
    "        self.r2_bn_1 = keras.layers.normalization.BatchNormalization()\n",
    "        self.r2_act_1 = Activation('relu')\n",
    "        self.r2_conv2d_1 = Conv2D(64, (3, 3), strides=[2,2], padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "        self.r2_bn_2 = keras.layers.normalization.BatchNormalization()\n",
    "        self.r2_act_2 = Activation('relu')\n",
    "        self.r2_conv2d_2 = Conv2D(64, (3, 3), padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "        self.r2_conv2d_3 = Conv2D(64, (1, 1), strides=[2,2], padding='same')\n",
    "        #x5 = add([x3, x4])\n",
    "\n",
    "        # Third residual block\n",
    "        self.r3_bn_1 = keras.layers.normalization.BatchNormalization()\n",
    "        self.r3_act_1 = Activation('relu')\n",
    "        self.r3_conv2d_1 = Conv2D(128, (3, 3), strides=[2,2], padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "        self.r3_bn_2 = keras.layers.normalization.BatchNormalization()\n",
    "        self.r3_act_2 = Activation('relu')\n",
    "        self.r3_conv2d_2 = Conv2D(128, (3, 3), padding='same',\n",
    "                                kernel_initializer=\"he_normal\",\n",
    "                                kernel_regularizer=regularizers.l2(1e-4))\n",
    "\n",
    "        self.r3_conv2d_3 = Conv2D(128, (1, 1), strides=[2,2], padding='same')\n",
    "        #x7 = add([x5, x6])\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(ResNet8, self).build(input_shape)  # Be sure to call this at the end\n",
    "        \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x1 = self.conv2d(input_tensor)\n",
    "        x1 = self.maxpool(x1)\n",
    "        \n",
    "        # First residual block\n",
    "        x2 = self.r1_bn_1(x1)\n",
    "        x2 = self.r1_act_1(x2)\n",
    "        x2 = self.r1_conv2d_1(x2)\n",
    "        \n",
    "        x2 = self.r1_bn_2(x2)\n",
    "        x2 = self.r1_act_2(x2)\n",
    "        x2 = self.r1_conv2d_2(x2)\n",
    "        \n",
    "        x1 = self.r1_conv2d_3(x1)\n",
    "        \n",
    "        x3 = add([x1, x2])\n",
    "        \n",
    "        # Second residual block\n",
    "        x4 = self.r2_bn_1(x3)\n",
    "        x4 = self.r2_act_1(x4)\n",
    "        x4 = self.r2_conv2d_1(x4)\n",
    "        \n",
    "        x4 = self.r2_bn_2(x4)\n",
    "        x4 = self.r2_act_2(x4)\n",
    "        x4 = self.r2_conv2d_2(x4)\n",
    "        \n",
    "        x3 = self.r2_conv2d_3(x3)\n",
    "        \n",
    "        x5 = add([x3, x4])\n",
    "        \n",
    "        # Third residual block\n",
    "        x6 = self.r3_bn_1(x5)\n",
    "        x6 = self.r3_act_1(x6)\n",
    "        x6 = self.r3_conv2d_1(x6)\n",
    "        \n",
    "        x6 = self.r3_bn_2(x6)\n",
    "        x6 = self.r3_act_2(x6)\n",
    "        x6 = self.r3_conv2d_2(x6)\n",
    "        \n",
    "        x5 = self.r3_conv2d_3(x5)\n",
    "        x7 = add([x5, x6])\n",
    "        \n",
    "        x = self.flatten(x7)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (2048,)\n",
    "        \n",
    "\n",
    "def pathPlanningModel(img_width, img_height, img_channels, \n",
    "                      len_spatial_history, num_classes, planning_horizon=25):\n",
    "    \"\"\"\n",
    "    Define model architecture.\n",
    "    \n",
    "    # Arguments\n",
    "       img_width: Target image widht.\n",
    "       img_height: Target image height.\n",
    "       img_channels: Target image channels.\n",
    "       \n",
    "    # Returns\n",
    "       model: A Model instance.\n",
    "    \"\"\"\n",
    "    #------------- Encoder: ResNet8  ---------------#\n",
    "    # Input\n",
    "    img_input = Input(shape=(img_height, img_width, img_channels))\n",
    "    nested_model = ResNet8()\n",
    "    y = nested_model(img_input)\n",
    "    \n",
    "    model = Model(inputs=[img_input], outputs=[y])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "resnet8_4 (ResNet8)          (2048,)                   309088    \n",
      "=================================================================\n",
      "Total params: 309,088\n",
      "Trainable params: 308,384\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = pathPlanningModel(128, 128, 1, 3, 400)\n",
    "# model.load_weights('./model/model_weights.h5', by_name=True)\n",
    "\n",
    "# # Freeze weights of encoder\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "This model decode the feature vector created by the Encoder to produce the prediction for the `PLANNING_HORIZON` (in form of classes of steering angles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathPlanningModel(encoder, len_spatial_history, img_width, img_height, img_channels, \n",
    "                      num_classes, planning_horizon=25):\n",
    "    \"\"\"\n",
    "    Define the model on top of resnet 8 which produces the steering angles applied to the PLANNING_HORIZON\n",
    "    \n",
    "    Input:\n",
    "        cnn_model (keras model): the feature extraction model\n",
    "        spatial_history (list of images): 3 frames with 4 meters between 2 adjacent frame\n",
    "        planning_horizion (int): number of output steering angles\n",
    "    \n",
    "    Output:\n",
    "        keras model instance \n",
    "    \"\"\"\n",
    "    # Input \n",
    "    spatial_history = Input(shape=(len_spatial_history, img_width, img_height, img_channels))\n",
    "    \n",
    "    # pass imgs in spatial_history through encoder\n",
    "    x = [encoder(spatial_history[i, :, :, :]) for i in range(len_spatial_history)]\n",
    "    \n",
    "    # concatenate x to make a single tensor\n",
    "    x = Concatenate()(x)\n",
    "    \n",
    "    # dropout\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # 1st Dense layer - 1164 units\n",
    "    x = Dense(1164, activation='relu')(x)\n",
    "    \n",
    "    # 2nd Dense layer - 500 units\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    \n",
    "    # 3rd Dense layer - num_classes units\n",
    "    x = Dense(num_classes, activation=None)(x)\n",
    "    \n",
    "    # softmax layer\n",
    "    y = [Activation('softmax')(x) for i in range(planning_horizon)]\n",
    "    \n",
    "    # concatenate y to make a single tensor\n",
    "    y = Concatenate()(y)\n",
    "    \n",
    "    # Define steering model\n",
    "    decoder = Model(inputs=[spatial_history], outputs=[x])\n",
    "    print(decoder.summary())\n",
    "\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: [<tf.Tensor 'model_3_3/flatten_3/Reshape:0' shape=(3, 6272) dtype=float32>, <tf.Tensor 'model_3_4/flatten_3/Reshape:0' shape=(3, 6272) dtype=float32>, <tf.Tensor 'model_3_5/flatten_3/Reshape:0' shape=(3, 6272) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d80369a5e407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathPlanningModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-e8f865a1cd17>\u001b[0m in \u001b[0;36mpathPlanningModel\u001b[0;34m(encoder, len_spatial_history, img_width, img_height, img_channels, num_classes, planning_horizon)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Define steering model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspatial_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                  \u001b[0;34m'the output of a Keras `Layer` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                                  \u001b[0;34m'(thus holding past layer metadata). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                  'Found: ' + str(x))\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         self._compute_previous_mask = (\n",
      "\u001b[0;31mValueError\u001b[0m: Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: [<tf.Tensor 'model_3_3/flatten_3/Reshape:0' shape=(3, 6272) dtype=float32>, <tf.Tensor 'model_3_4/flatten_3/Reshape:0' shape=(3, 6272) dtype=float32>, <tf.Tensor 'model_3_5/flatten_3/Reshape:0' shape=(3, 6272) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "decoder = pathPlanningModel(model, 3, 200, 200, 1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
