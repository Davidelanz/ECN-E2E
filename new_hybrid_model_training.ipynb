{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from new_model.model_utils import resnet_shorten, hybrid_LSTM_training, _classifier, convolutional_block\n",
    "from keras.layers import LSTM, Reshape, Input, Conv2D, MaxPooling2D, Lambda\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from training_utils import save_model, DataGenerator, generate_dataset\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (199, 265, 1)\n",
    "LSTM_DIM_HIDDEN = 64\n",
    "LEN_SPATIAL_HISTORY = 4\n",
    "NUM_CLASS = 73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name of previously trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_encoder_file = \"shared_encoder_resnet8_2019_05_22_11_04.h5\"\n",
    "# separate_encoder_files = [\"sep_encoder_%d_2019_05_22_11_04.h5\" % i for i in range(LEN_SPATIAL_HISTORY)]\n",
    "# shared_lstm_file = \".p\"\n",
    "shared_classifier_file = \"classifier_2019_05_22_11_04.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 2.Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**------------------------**\n",
    "## 2.1 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Shared encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/user/venv/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:210: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "shared_encoder = resnet_shorten(IMG_SHAPE, model_name=\"shared_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 199, 265, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 100, 133, 32) 832         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 49, 66, 32)   0           conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_1_a (BatchNormalization)     (None, 49, 66, 32)   128         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 49, 66, 32)   0           bn_1_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_a (Conv2D)               (None, 25, 33, 32)   9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn_1_b (BatchNormalization)     (None, 25, 33, 32)   128         conv_1_a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 25, 33, 32)   0           bn_1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_b (Conv2D)               (None, 25, 33, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_c (Conv2D)               (None, 25, 33, 32)   1056        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 25, 33, 32)   0           conv_1_b[0][0]                   \n",
      "                                                                 conv_1_c[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn_2_a (BatchNormalization)     (None, 25, 33, 32)   128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 25, 33, 32)   0           bn_2_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_a (Conv2D)               (None, 13, 17, 64)   18496       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn_2_b (BatchNormalization)     (None, 13, 17, 64)   256         conv_2_a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 13, 17, 64)   0           bn_2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_b (Conv2D)               (None, 13, 17, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_c (Conv2D)               (None, 13, 17, 64)   2112        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 13, 17, 64)   0           conv_2_b[0][0]                   \n",
      "                                                                 conv_2_c[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 78,560\n",
      "Trainable params: 78,240\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "shared_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Separate encoder\n",
    "\n",
    "This is a convolution block of ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _separate_encoder(input_shape, model_name):\n",
    "    \"\"\"\n",
    "    Create a model from the function named \"convolutional_block\". This model is later used as a layer\n",
    "    in the full hybrid model\n",
    "    \n",
    "    Input: \n",
    "        input_shape (tuple): shape of feature vectors created by shared_encoder\n",
    "        num_filters (list): number of filters of each Conv2D layer of this model\n",
    "        strides (list): size of strides of each Conv2D layer\n",
    "        stage: must set to be None\n",
    "        model_name (str):\n",
    "        \n",
    "    Output:\n",
    "        keras Model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # define input\n",
    "    X_input = Input(shape=input_shape)\n",
    "    \n",
    "    X = Conv2D(128, (3, 3), padding='valid', strides=(2, 2))(X_input)\n",
    "    X = MaxPooling2D()(X)\n",
    "    X = Conv2D(256, (3, 3), padding='valid', strides=(2, 2))(X)\n",
    "    X = Reshape((1, -1))(X)\n",
    "    # define model\n",
    "    model = Model(inputs=[X_input], outputs=[X], name=model_name)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config of separate encoder\n",
    "s_input_shape = (13, 17, 64)  # shape of output of shared encoder\n",
    "\n",
    "sep_encoder_list = [_separate_encoder(s_input_shape, model_name=\"sep_en_%d\" % i) \n",
    "                    for i in range(LEN_SPATIAL_HISTORY)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 13, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 6, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 1, 256)         295168    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 256)            0         \n",
      "=================================================================\n",
      "Total params: 369,024\n",
      "Trainable params: 369,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sep_encoder_list[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**------------------------**\n",
    "## 2.2 Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_cell = LSTM(LSTM_DIM_HIDDEN, return_sequences=True)\n",
    "LSTM_cell_2 = LSTM(LSTM_DIM_HIDDEN, return_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Define Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = _classifier(input_shape=(LSTM_DIM_HIDDEN, ), num_class=NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 73)                18761     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 73)                0         \n",
      "=================================================================\n",
      "Total params: 35,401\n",
      "Trainable params: 35,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_v3(image_shape, shared_encoder, sep_encoder_list, LSTM_cell_list, classifier, Ty):\n",
    "    \"\"\"\n",
    "    Architect: image -> shared_encoder -> separate_encoder -> lstm_1 -> lstm_2 -> shared_classifier -> y\n",
    "    \n",
    "    Input:\n",
    "        image_shape (tuple): shape of input image\n",
    "        shared_encoder (keras.Model): shared model used to extract low level feature vector from input image\n",
    "        sep_encoder_list (list): list of keras.Model storing separate encoder\n",
    "        LSTM_cell (keras.layers)\n",
    "        classifier (keras.Model): shared classifier to predict class of steering angle\n",
    "        Ty (int): length of spatial history\n",
    "        \n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    X_input_list = [Input(shape=image_shape) for i in range(Ty)]\n",
    "    \n",
    "    # pass each input through shared encoder\n",
    "    shared_encoded_X = [shared_encoder(X) for X in X_input_list]\n",
    "    \n",
    "    # pass each encoded_X through its own convolution block\n",
    "    separate_encoded_X = [separate_encoder(X) \n",
    "                          for separate_encoder, X in zip(sep_encoder_list, shared_encoded_X)]\n",
    "    \n",
    "    # concatenate encoded vector\n",
    "    X = keras.layers.concatenate(separate_encoded_X, axis=1)\n",
    "    \n",
    "    # 1st LSTM layer\n",
    "    X = LSTM_cell_list[0](X)  # output shape (LEN_SPATIAL_HISTORY, LSTM_DIM_HIDDEN)\n",
    "    X = LSTM_cell_list[1](X)\n",
    "    \n",
    "    # propagate through classifier\n",
    "    outputs = []\n",
    "    for i in range(LEN_SPATIAL_HISTORY):\n",
    "        X_i = Lambda(lambda x: x[:,i,:])(X)  # slice tensor X\n",
    "        # invoke classifier\n",
    "        outputs.append(classifier(X_i))\n",
    "    \n",
    "    # define model\n",
    "    model = Model(inputs=X_input_list, outputs=outputs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hybrid_model = model_v3(IMG_SHAPE, shared_encoder, sep_encoder_list, [LSTM_cell, LSTM_cell_2], classifier, LEN_SPATIAL_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 199, 265, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 199, 265, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 199, 265, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 199, 265, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_encoder (Model)          (None, 13, 17, 64)   78560       input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sep_en_0 (Model)                (None, 1, 256)       369024      shared_encoder[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sep_en_1 (Model)                (None, 1, 256)       369024      shared_encoder[2][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sep_en_2 (Model)                (None, 1, 256)       369024      shared_encoder[3][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sep_en_3 (Model)                (None, 1, 256)       369024      shared_encoder[4][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 256)       0           sep_en_0[1][0]                   \n",
      "                                                                 sep_en_1[1][0]                   \n",
      "                                                                 sep_en_2[1][0]                   \n",
      "                                                                 sep_en_3[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 4, 64)        82176       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 4, 64)        33024       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 73)           35401       lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,705,257\n",
      "Trainable params: 1,704,937\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Load weights & compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shared_encoder\n",
    "shared_encoder.load_weights(\"./new_model/weights/shared_encoder/%s\" % shared_encoder_file, by_name=True)\n",
    "\n",
    "# # Load shared LSTM\n",
    "# with open('./new_model/weights/shared_lstm/%s' % shared_lstm_file, 'rb') as fp:\n",
    "#     lstm_weights_dict = pickle.load(fp)\n",
    "    \n",
    "# lstm_weights = []\n",
    "# for k in lstm_weights_dict.keys():\n",
    "#     lstm_weights.append(lstm_weights_dict[k])\n",
    "    \n",
    "# LSTM_cell.set_weights(lstm_weights)\n",
    "\n",
    "# # Load separate encoder\n",
    "# for sep_enc_file, sep_enc in zip(separate_encoder_files, sep_encoder_list):\n",
    "#     sep_enc.load_weights(\"./new_model/weights/separate_encoder/%s\" % sep_enc_file)\n",
    "\n",
    "# Load shared classifier\n",
    "# classifier.load_weights(\"./new_model/weights/shared_classifier/%s\" % shared_classifier_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "otim = keras.optimizers.Adam(lr=0.35, decay=0.001)\n",
    "hybrid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "param_train = {'img_shape': IMG_SHAPE, \n",
    "             'Ty': LEN_SPATIAL_HISTORY, \n",
    "             'num_class': NUM_CLASS, \n",
    "             'batch_size': batch_size, \n",
    "             'shuffle': True, \n",
    "             'additional_input_for_LSTM': False, \n",
    "             'LSTM_dim_hidden_states': LSTM_DIM_HIDDEN}\n",
    "\n",
    "train_gen = DataGenerator(\"./new_data/widthen_bin_training_CH2_only.csv\", **param_train)\n",
    "\n",
    "param_val = {'img_shape': IMG_SHAPE, \n",
    "             'num_class': NUM_CLASS, \n",
    "             'Ty': LEN_SPATIAL_HISTORY, \n",
    "             'LSTM_dim_hidden_states': LSTM_DIM_HIDDEN, \n",
    "             'additional_input_for_LSTM': False,\n",
    "             'color_img': False}\n",
    "X_val, y_val = generate_dataset(\"./new_data/widthen_bin_validation_CH2_only.csv\", **param_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./logs/' + time_str,  \n",
    "                                          batch_size=batch_size, \n",
    "                                          update_freq='epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "569/569 [==============================] - 234s 411ms/step - loss: 8.7410 - model_1_loss: 2.1067 - model_1_acc: 0.3226 - model_1_acc_1: 0.3299 - model_1_acc_2: 0.3479 - model_1_acc_3: 0.3791 - val_loss: 8.8232 - val_model_1_loss: 2.1259 - val_model_1_acc: 0.3235 - val_model_1_acc_1: 0.3368 - val_model_1_acc_2: 0.3486 - val_model_1_acc_3: 0.3738\n",
      "Epoch 22/30\n",
      "569/569 [==============================] - 232s 407ms/step - loss: 8.7225 - model_1_loss: 2.0974 - model_1_acc: 0.3238 - model_1_acc_1: 0.3343 - model_1_acc_2: 0.3521 - model_1_acc_3: 0.3836 - val_loss: 8.8362 - val_model_1_loss: 2.1403 - val_model_1_acc: 0.2948 - val_model_1_acc_1: 0.3146 - val_model_1_acc_2: 0.3452 - val_model_1_acc_3: 0.3501\n",
      "Epoch 23/30\n",
      "569/569 [==============================] - 234s 411ms/step - loss: 8.7015 - model_1_loss: 2.0854 - model_1_acc: 0.3255 - model_1_acc_1: 0.3349 - model_1_acc_2: 0.3518 - model_1_acc_3: 0.3864 - val_loss: 8.7709 - val_model_1_loss: 2.1067 - val_model_1_acc: 0.3314 - val_model_1_acc_1: 0.3343 - val_model_1_acc_2: 0.3585 - val_model_1_acc_3: 0.3723\n",
      "Epoch 24/30\n",
      "569/569 [==============================] - 235s 413ms/step - loss: 8.6850 - model_1_loss: 2.0763 - model_1_acc: 0.3248 - model_1_acc_1: 0.3367 - model_1_acc_2: 0.3578 - model_1_acc_3: 0.3907 - val_loss: 8.7733 - val_model_1_loss: 2.1124 - val_model_1_acc: 0.3314 - val_model_1_acc_1: 0.3358 - val_model_1_acc_2: 0.3516 - val_model_1_acc_3: 0.3694\n",
      "Epoch 25/30\n",
      "569/569 [==============================] - 233s 410ms/step - loss: 8.6795 - model_1_loss: 2.0723 - model_1_acc: 0.3221 - model_1_acc_1: 0.3327 - model_1_acc_2: 0.3554 - model_1_acc_3: 0.3873 - val_loss: 8.7567 - val_model_1_loss: 2.0998 - val_model_1_acc: 0.3348 - val_model_1_acc_1: 0.3373 - val_model_1_acc_2: 0.3560 - val_model_1_acc_3: 0.3783\n",
      "Epoch 26/30\n",
      "569/569 [==============================] - 234s 412ms/step - loss: 8.6582 - model_1_loss: 2.0619 - model_1_acc: 0.3251 - model_1_acc_1: 0.3384 - model_1_acc_2: 0.3582 - model_1_acc_3: 0.3950 - val_loss: 8.7377 - val_model_1_loss: 2.0858 - val_model_1_acc: 0.3309 - val_model_1_acc_1: 0.3393 - val_model_1_acc_2: 0.3541 - val_model_1_acc_3: 0.3793\n",
      "Epoch 27/30\n",
      "569/569 [==============================] - 233s 410ms/step - loss: 8.6431 - model_1_loss: 2.0534 - model_1_acc: 0.3227 - model_1_acc_1: 0.3375 - model_1_acc_2: 0.3579 - model_1_acc_3: 0.3961 - val_loss: 8.7161 - val_model_1_loss: 2.0826 - val_model_1_acc: 0.3309 - val_model_1_acc_1: 0.3378 - val_model_1_acc_2: 0.3644 - val_model_1_acc_3: 0.3807\n",
      "Epoch 28/30\n",
      "569/569 [==============================] - 240s 421ms/step - loss: 8.6373 - model_1_loss: 2.0483 - model_1_acc: 0.3254 - model_1_acc_1: 0.3360 - model_1_acc_2: 0.3609 - model_1_acc_3: 0.3974 - val_loss: 8.6990 - val_model_1_loss: 2.0680 - val_model_1_acc: 0.3274 - val_model_1_acc_1: 0.3348 - val_model_1_acc_2: 0.3541 - val_model_1_acc_3: 0.3995\n",
      "Epoch 29/30\n",
      "569/569 [==============================] - 237s 416ms/step - loss: 8.6220 - model_1_loss: 2.0401 - model_1_acc: 0.3245 - model_1_acc_1: 0.3372 - model_1_acc_2: 0.3613 - model_1_acc_3: 0.4005 - val_loss: 8.6967 - val_model_1_loss: 2.0636 - val_model_1_acc: 0.3274 - val_model_1_acc_1: 0.3393 - val_model_1_acc_2: 0.3689 - val_model_1_acc_3: 0.3990\n",
      "Epoch 30/30\n",
      "569/569 [==============================] - 238s 418ms/step - loss: 8.6073 - model_1_loss: 2.0313 - model_1_acc: 0.3230 - model_1_acc_1: 0.3363 - model_1_acc_2: 0.3618 - model_1_acc_3: 0.4037 - val_loss: 8.7003 - val_model_1_loss: 2.0692 - val_model_1_acc: 0.3254 - val_model_1_acc_1: 0.3185 - val_model_1_acc_2: 0.3600 - val_model_1_acc_3: 0.3822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef728954a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model.fit_generator(train_gen,\n",
    "                           epochs=30,\n",
    "                           validation_data=(X_val, y_val),\n",
    "                           initial_epoch=20,\n",
    "                           callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save shared_encoder \n",
    "shared_encoder.save_weights(\"./new_model/weights/shared_encoder/shared_encoder_resnet8_%s.h5\" % time_str)\n",
    "\n",
    "# save shared_lstm\n",
    "save_lstm(LSTM_cell, 1)\n",
    "save_lstm(LSTM_cell_2, 2)\n",
    "\n",
    "# save separate encoder\n",
    "for i, sep_encoder in enumerate(sep_encoder_list):\n",
    "    sep_encoder.save_weights(\"./new_model/weights/separate_encoder/sep_encoder_v3_%d_%s.h5\" % (i, time_str))\n",
    "\n",
    "# save classifier\n",
    "classifier.save_weights(\"./new_model/weights/shared_classifier/classifier_%s.h5\" % time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
